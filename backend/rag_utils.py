"""
rag_utils.py
-------------
Text-based RAG utilities for Qwen2.5-VL.

Purpose:
    - Retrieve fashion item images + descriptions based on a textual recommendation
      generated by the model.
    - Used by backend_api.py after Qwen outputs its fashion advice.

Key Components:
    • init_rag() — lazy initialization of ChromaDB, SentenceTransformer, TF-IDF, and image store.
    • hybrid_retrieve_v2() — hybrid semantic + keyword + type retrieval.
    • retrieve_relevant_items_from_text() — one-call wrapper returning images + metadata.

Dependencies:
    chromadb, sentence-transformers, scikit-learn, Pillow
"""

import os
import pickle
import chromadb
import torch
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from PIL import Image

# ---------------------------------------------------------------------
# Global lazy objects
# ---------------------------------------------------------------------
_chroma_client = None
_collection = None
_embedding_model = None
_tfidf_vectorizer = None
_image_store = None

# Default paths
_DEFAULT_DB_PATH = "./chroma_fashion_db_hybrid"
_DEFAULT_IMAGE_STORE_PATH = "fashion_image_store_hybrid.pkl"


# ---------------------------------------------------------------------
# Initialization
# ---------------------------------------------------------------------
def init_rag(
    db_path: str = _DEFAULT_DB_PATH,
    image_store_path: str = _DEFAULT_IMAGE_STORE_PATH,
) -> None:
    """
    Initialize RAG components. 
    If Chroma or image store are missing, automatically download and build them
    using the Anony100/FashionRec dataset from Hugging Face.
    """
    global _chroma_client, _collection, _embedding_model, _tfidf_vectorizer, _image_store

    from datasets import load_dataset

    # --- Step 1: ChromaDB connection or creation ---
    if _chroma_client is None:
        os.makedirs(db_path, exist_ok=True)
        _chroma_client = chromadb.PersistentClient(path=db_path)

    # Try to load collection; if missing, build from dataset
    try:
        _collection = _chroma_client.get_collection("fashion_items_hybrid")
        print("✓ Loaded existing Chroma collection")
    except Exception:
        print("⚠ No existing collection found — building from dataset...")
        _collection = _chroma_client.create_collection("fashion_items_hybrid")

        # Download dataset from Hugging Face (FashionRec)
        dataset = load_dataset("Anony100/FashionRec", split="train")

        # Initialize embedding model
        _embedding_model = SentenceTransformer("all-mpnet-base-v2")

        # Prepare batches for embedding & Chroma ingestion
        batch_size = 64
        ids, docs, metas = [], [], []
        image_store = {}

        from tqdm import tqdm
        from PIL import Image
        import requests
        from io import BytesIO

        for i in tqdm(range(0, len(dataset), batch_size), desc="Building Chroma collection"):
            batch = dataset[i:i + batch_size]

            for item in batch:
                item_id = str(item.get("id", len(ids)))
                caption = item.get("caption", "")
                keywords = item.get("category", "")
                image_url = item.get("image_url") or item.get("image")

                # Save text info
                ids.append(item_id)
                docs.append(caption)
                metas.append({"keywords": keywords, "url": image_url})

                # Try to store image (for retrieval visuals)
                try:
                    img = Image.open(BytesIO(requests.get(image_url, timeout=10).content)).convert("RGB")
                    img = img.resize((224, 224))
                    image_store[item_id] = img
                except Exception:
                    continue

        # Compute embeddings
        print("Encoding embeddings...")
        embeddings = _embedding_model.encode(docs, batch_size=64, show_progress_bar=True).tolist()

        # Insert into Chroma
        print("Populating Chroma collection...")
        _collection.add(ids=ids, documents=docs, metadatas=metas, embeddings=embeddings)

        # Save image store
        with open(image_store_path, "wb") as f:
            pickle.dump(image_store, f)

        print(f"✓ Built new ChromaDB and image store with {len(ids)} items")

    # --- Step 2: Load existing Chroma if found ---
    if _collection is None:
        _collection = _chroma_client.get_collection("fashion_items_hybrid")

    # --- Step 3: Embedding model & TF-IDF ---
    if _embedding_model is None:
        _embedding_model = SentenceTransformer("all-mpnet-base-v2")

    if _tfidf_vectorizer is None:
        _tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words="english")
        docs = _collection.get(limit=min(_collection.count(), 3000)).get("documents", [])
        if docs:
            _tfidf_vectorizer.fit(docs)

    # --- Step 4: Load image store ---
    if _image_store is None:
        if os.path.exists(image_store_path):
            with open(image_store_path, "rb") as f:
                _image_store = pickle.load(f)
        else:
            _image_store = {}

# ---------------------------------------------------------------------
# Hybrid Retrieval
# ---------------------------------------------------------------------
def hybrid_retrieve_v2(
    query_text: str,
    top_k: int = 3,
    semantic_weight: float = 0.6,
    keyword_weight: float = 0.25,
    type_weight: float = 0.15,
) -> List[Dict[str, Any]]:
    """
    Hybrid retrieval (semantic + keyword + lexical overlap).

    Returns:
        List of dicts containing:
            item_id, description, score, semantic_score, keyword_overlap, type_overlap
    """
    if _collection is None or _embedding_model is None:
        raise RuntimeError("RAG not initialized. Call init_rag() first.")

    # Encode query to vector
    query_embedding = _embedding_model.encode(query_text).tolist()

    # Query Chroma for nearest embeddings
    semantic_results = _collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k * 3,
    )

    # Extract words for keyword matching
    query_keywords = set([w for w in query_text.lower().split() if len(w) > 3])

    scored = []
    for idx, (iid, desc, meta) in enumerate(
        zip(
            semantic_results["ids"][0],
            semantic_results["documents"][0],
            semantic_results["metadatas"][0],
        )
    ):
        # Convert distance to similarity-like score
        semantic_score = 1.0 - semantic_results["distances"][0][idx]

        # Keyword overlap (TF-IDF terms from metadata)
        item_keywords = set((meta or {}).get("keywords", "").split(","))
        keyword_overlap = len(query_keywords & item_keywords) / (
            len(query_keywords) + len(item_keywords) + 1e-6
        )

        # Lexical/type overlap between query and description
        try:
            desc_words = set(desc.lower().split())
            q_words = set(query_text.lower().split())
            type_overlap = len(desc_words & q_words) / (len(q_words) + 1e-6)
        except Exception:
            type_overlap = 0.0

        # Weighted hybrid score
        hybrid_score = (
            semantic_weight * semantic_score
            + keyword_weight * keyword_overlap
            + type_weight * type_overlap
        )

        scored.append(
            {
                "item_id": iid,
                "description": desc,
                "score": hybrid_score,
                "semantic_score": semantic_score,
                "keyword_overlap": keyword_overlap,
                "type_overlap": type_overlap,
            }
        )

    scored.sort(key=lambda x: x["score"], reverse=True)
    return scored[:top_k]


# ---------------------------------------------------------------------
# Public entry point
# ---------------------------------------------------------------------
def retrieve_relevant_items_from_text(
    recommendation_text: str,
    top_k: int = 3,
) -> Dict[str, Any]:
    """
    Retrieve visually and semantically similar fashion items for a given recommendation string.

    Example:
        recommendation_text = "A black shirt top will fit perfectly."
        -> returns 3-6 black shirt images + metadata.

    Returns:
        {
            "rag_results": [ {...}, {...}, ... ],
            "images": [PIL.Image or None, ...]
        }
    """
    init_rag()

    results = hybrid_retrieve_v2(recommendation_text, top_k=top_k)
    images = []
    for r in results:
        img = _image_store.get(r["item_id"]) if _image_store else None
        images.append(img if isinstance(img, Image.Image) else None)

    return {"rag_results": results, "images": images}
