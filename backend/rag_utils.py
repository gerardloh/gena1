"""
rag_utils.py
-------------
Text-based RAG utilities for Qwen2.5-VL.

Purpose:
    - Retrieve fashion item images + descriptions based on a textual recommendation
      generated by the model.
    - Used by backend_api.py after Qwen outputs its fashion advice.

Key Components:
    • init_rag() — lazy initialization of ChromaDB, SentenceTransformer, TF-IDF, and image store.
    • hybrid_retrieve_v2() — hybrid semantic + keyword + type retrieval.
    • retrieve_relevant_items_from_text() — one-call wrapper returning images + metadata.

Dependencies:
    chromadb, sentence-transformers, scikit-learn, Pillow
"""

import os
import pickle
import chromadb
import torch
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from PIL import Image

# ---------------------------------------------------------------------
# Global lazy objects
# ---------------------------------------------------------------------
_chroma_client = None
_collection = None
_embedding_model = None
_tfidf_vectorizer = None
_image_store = None

# Default paths
_DEFAULT_DB_PATH = "./chroma_fashion_db_hybrid"
_DEFAULT_IMAGE_STORE_PATH = "fashion_image_store_hybrid.pkl"


# ---------------------------------------------------------------------
# Initialization
# ---------------------------------------------------------------------
def init_rag(
    db_path: str = _DEFAULT_DB_PATH,
    image_store_path: str = _DEFAULT_IMAGE_STORE_PATH,
) -> None:
    """
    Initialize ChromaDB client, embedding model, TF-IDF, and image store.

    Safe to call multiple times — initialization is idempotent.
    """
    global _chroma_client, _collection, _embedding_model, _tfidf_vectorizer, _image_store

    # --- ChromaDB connection ---
    if _chroma_client is None:
        _chroma_client = chromadb.PersistentClient(path=db_path)
        _collection = _chroma_client.get_collection("fashion_items_hybrid")

    # --- Embedding model ---
    if _embedding_model is None:
        _embedding_model = SentenceTransformer("all-mpnet-base-v2")

    # --- TF-IDF vectorizer (keyword weighting) ---
    if _tfidf_vectorizer is None:
        _tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words="english")
        try:
            docs = _collection.get(limit=min(_collection.count(), 3000)).get("documents", [])
            if docs:
                _tfidf_vectorizer.fit(docs)
        except Exception:
            # Continue even if fitting fails (hybrid retrieval will degrade gracefully)
            pass

    # --- Image store (item_id → PIL.Image) ---
    if _image_store is None:
        if os.path.exists(image_store_path):
            with open(image_store_path, "rb") as f:
                _image_store = pickle.load(f)
        else:
            _image_store = {}


# ---------------------------------------------------------------------
# Hybrid Retrieval
# ---------------------------------------------------------------------
def hybrid_retrieve_v2(
    query_text: str,
    top_k: int = 3,
    semantic_weight: float = 0.6,
    keyword_weight: float = 0.25,
    type_weight: float = 0.15,
) -> List[Dict[str, Any]]:
    """
    Hybrid retrieval (semantic + keyword + lexical overlap).

    Returns:
        List of dicts containing:
            item_id, description, score, semantic_score, keyword_overlap, type_overlap
    """
    if _collection is None or _embedding_model is None:
        raise RuntimeError("RAG not initialized. Call init_rag() first.")

    # Encode query to vector
    query_embedding = _embedding_model.encode(query_text).tolist()

    # Query Chroma for nearest embeddings
    semantic_results = _collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k * 3,
    )

    # Extract words for keyword matching
    query_keywords = set([w for w in query_text.lower().split() if len(w) > 3])

    scored = []
    for idx, (iid, desc, meta) in enumerate(
        zip(
            semantic_results["ids"][0],
            semantic_results["documents"][0],
            semantic_results["metadatas"][0],
        )
    ):
        # Convert distance to similarity-like score
        semantic_score = 1.0 - semantic_results["distances"][0][idx]

        # Keyword overlap (TF-IDF terms from metadata)
        item_keywords = set((meta or {}).get("keywords", "").split(","))
        keyword_overlap = len(query_keywords & item_keywords) / (
            len(query_keywords) + len(item_keywords) + 1e-6
        )

        # Lexical/type overlap between query and description
        try:
            desc_words = set(desc.lower().split())
            q_words = set(query_text.lower().split())
            type_overlap = len(desc_words & q_words) / (len(q_words) + 1e-6)
        except Exception:
            type_overlap = 0.0

        # Weighted hybrid score
        hybrid_score = (
            semantic_weight * semantic_score
            + keyword_weight * keyword_overlap
            + type_weight * type_overlap
        )

        scored.append(
            {
                "item_id": iid,
                "description": desc,
                "score": hybrid_score,
                "semantic_score": semantic_score,
                "keyword_overlap": keyword_overlap,
                "type_overlap": type_overlap,
            }
        )

    scored.sort(key=lambda x: x["score"], reverse=True)
    return scored[:top_k]


# ---------------------------------------------------------------------
# Public entry point
# ---------------------------------------------------------------------
def retrieve_relevant_items_from_text(
    recommendation_text: str,
    top_k: int = 3,
) -> Dict[str, Any]:
    """
    Retrieve visually and semantically similar fashion items for a given recommendation string.

    Example:
        recommendation_text = "A black shirt top will fit perfectly."
        -> returns 3-6 black shirt images + metadata.

    Returns:
        {
            "rag_results": [ {...}, {...}, ... ],
            "images": [PIL.Image or None, ...]
        }
    """
    init_rag()

    results = hybrid_retrieve_v2(recommendation_text, top_k=top_k)
    images = []
    for r in results:
        img = _image_store.get(r["item_id"]) if _image_store else None
        images.append(img if isinstance(img, Image.Image) else None)

    return {"rag_results": results, "images": images}
